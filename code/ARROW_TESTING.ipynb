{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "kggpv2m7svrc5vxpkoz5",
   "authorId": "259978707858",
   "authorName": "MLFCTO_USER",
   "authorEmail": "harsh.patel@snowflake.com",
   "sessionId": "543fbdd1-bd3a-417d-abeb-c718b6d4a6a3",
   "lastEditTime": 1747667860896
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b3101e-cc5e-4a16-aaff-5420a09f9f89",
   "metadata": {
    "name": "ARROW_DECIMAL_TITLE",
    "collapsed": false
   },
   "source": "# Testing Arrow Decimal\n## Design:\n### 1. Test individual logic - Snowflake data downcast( DECIMAL(37,18) ), PyArrow operand creation, PyArrow multiply().\n### 2. Confirm they meet response time and identify bottlenecks and optimizations.\n### 3. Sum of testing logic could be registered as a Snowflake Model Object.\n        Model Object can run an SPCS Inferencing Service that is meant for real-time potentially avoiding UDF Python Construction / Deconstruction of the Environment which is the primary wall time on small Decimal Arrow Operations.\n        "
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "package_mgmt"
   },
   "source": "# Standard Python Libraries\nimport pandas as pd\n\n# PyArrow for High-Performance Computing\nimport pyarrow as pa\nimport pyarrow.compute as pc\n\n# Snowflake Snowpark (DataFrame API for Snowflake)\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col\n\n# Initialize Snowflake session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3aeeb52e-e721-4379-9c85-b80d0c5b5525",
   "metadata": {
    "language": "sql",
    "name": "SETUP_SYNTHETIC_DATA"
   },
   "outputs": [],
   "source": "-- Setting Up Environment for Testing Downcasting Techniques in Snowflake --\n\n-- Step 1: Set User Role and Access Permissions --\n-- Granting all privileges on the database to the user role (only necessary if not previously configured) --\n-- USE ROLE ACCOUNTADMIN;\n-- GRANT ALL PRIVILEGES ON DATABASE CONTAINER_RUNTIME_LAB TO ROLE CONTAINER_RUNTIME_LAB_USER;\n\n-- Step 2: Switch to the User Role and Database for Testing --\nUSE ROLE CONTAINER_RUNTIME_LAB_USER;\nUSE DATABASE CONTAINER_RUNTIME_LAB;\n\n-- Step 3: Create a Dedicated Schema for Testing (Arrow) --\nCREATE OR REPLACE SCHEMA ARROW;\n\n-- Step 4: Create Sample Table for Downcasting Tests --\nCREATE OR REPLACE TABLE SAMPLE_TBL (\n    col_a DECIMAL(38,2),  -- High precision DECIMAL column for testing downcasting\n    col_b DECIMAL(38,2)   -- Another DECIMAL column for multi-column testing\n);\n-- DECIMAL(38,12)\n-- Step 5: Populate Sample Table with Random Test Data (2000 Rows) --\n-- Using Snowflake's GENERATOR function for fast data creation --\nINSERT INTO SAMPLE_TBL (col_a, col_b)\nSELECT \n    ROUND(RANDOM() * 9999999999999.99, 1) AS col_a,  -- Random decimal values with 1 decimal place\n    ROUND(RANDOM() * 9999999999999.99, 1) AS col_b\nFROM \n    TABLE(GENERATOR(ROWCOUNT => 2000));\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3648868-caab-4ef5-aef9-5a8131b60e56",
   "metadata": {
    "language": "sql",
    "name": "CAST_TABLE"
   },
   "outputs": [],
   "source": "-- Testing Various Techniques for Downcasting DECIMAL Columns in Snowflake --\n\n-- New Table Creation Method for Downcasting DECIMAL Precision and Scale --\n-- This approach creates a new table with the downcasted column, preserving data integrity. --\n-- Recommended: Use Snowpark for Lazy Evaluation in DAG Execution (Efficient and Scalable). --\n\n-- Step 1: Create a New Table with Downcasted Column (DECIMAL(35,2)) --\nCREATE OR REPLACE TABLE SAMPLE_TBL_NEW AS \nSELECT \n    CAST(COL_A AS DECIMAL(37,2)) AS COL_A,  -- Downcasting COL_A to DECIMAL(35,2)\n    -- Explicitly list other columns to maintain structure --\n    COL_B  \nFROM \n    SAMPLE_TBL;\n\n-- Step 2: Swap Tables (Instant Metadata Operation) --\nALTER TABLE SAMPLE_TBL RENAME TO SAMPLE_TBL_OLD;  -- Rename original table for backup\nALTER TABLE SAMPLE_TBL_NEW RENAME TO SAMPLE_TBL;  -- Replace with the new downcasted version\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0a64a3a-3984-4e8e-a44b-ec6e83f938d2",
   "metadata": {
    "language": "sql",
    "name": "CAST_TABLE_OUTPUT"
   },
   "outputs": [],
   "source": "DROP TABLE SAMPLE_TBL_OLD;                        -- Drop the backup table (Cleanup)\n\n-- Verify the Precision and Scale of the Downcasted Column --\nSELECT \n    COLUMN_NAME, \n    DATA_TYPE, \n    NUMERIC_PRECISION, \n    NUMERIC_SCALE \nFROM \n    INFORMATION_SCHEMA.COLUMNS \nWHERE \n    TABLE_NAME = 'SAMPLE_TBL' \n    AND COLUMN_NAME = 'COL_A';\n-- Note: INFORMATION_SCHEMA may temporarily show multiple column versions due to metadata updates.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0eb8758-f14b-40d2-83cc-58304bf0bf9d",
   "metadata": {
    "language": "sql",
    "name": "CAST_VIEW"
   },
   "outputs": [],
   "source": "-- Testing Various Techniques for Downcasting DECIMAL Columns in Snowflake --\n\n-- View Creation Method for Downcasting DECIMAL Precision and Scale --\n-- This approach creates a view that dynamically applies the downcast without modifying the original table data. \n\n-- Recommended: Use Snowpark for Lazy Evaluation in DAG Execution (Efficient and Scalable). --\nCREATE OR REPLACE VIEW SAMPLE_TBL_MAT_VIEW AS \nSELECT \n    CAST(COL_A AS DECIMAL(36,2)) AS COL_A,  -- Downcasting COL_A to DECIMAL(35,2)\n    -- Explicitly list other columns to maintain structure --\n    COL_B  \nFROM \n    SAMPLE_TBL;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ddbd323-c08f-40fb-85f3-1602b93faaf4",
   "metadata": {
    "language": "sql",
    "name": "CAST_VIEW_OUTPUT"
   },
   "outputs": [],
   "source": "-- Verifying the Precision and Scale of the Downcasted Column in the View --\nSELECT \n    COLUMN_NAME, \n    DATA_TYPE, \n    NUMERIC_PRECISION, \n    NUMERIC_SCALE \nFROM \n    INFORMATION_SCHEMA.COLUMNS \nWHERE \n    TABLE_NAME = 'SAMPLE_TBL_MAT_VIEW' \n    -- AND COLUMN_NAME = 'COL_A';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50e6bb18-ec5d-43bf-b462-4fce8b9d4383",
   "metadata": {
    "language": "python",
    "name": "SNOWPARK_CAST_VIEW"
   },
   "outputs": [],
   "source": "# Snowpark version of DOWNCAST_CREATE_VIEW\n# There is no create_or_replace_view with options like casting columns in Snowpark Dataframe API\n# See Dataframe Class - https://github.com/snowflakedb/snowpark-python/blob/0511a45947242ae1f7deb18126886d59bc711926/src/snowflake/snowpark/dataframe.py#L4992\n\n# Using Snowpark session object for setting role and database\nsession.use_database(\"CONTAINER_RUNTIME_LAB\")\nsession.use_schema(\"ARROW\")\n\n# Creating the View Using Direct SQL (Downcasting Column)\nsession.sql(\"\"\"\nCREATE OR REPLACE VIEW SAMPLE_TBL_SNOW_VIEW AS \nSELECT \n    CAST(COL_A AS DECIMAL(38,2)) AS COL_A,  -- Downcasting COL_A to DECIMAL(35,2)\n    COL_B  \nFROM \n    SAMPLE_TBL;\n\"\"\").collect()\n\n# Loading the Created View as a Snowpark DataFrame\nsample_tbl_view_df = session.table(\"CONTAINER_RUNTIME_LAB.ARROW.SAMPLE_TBL_SNOW_VIEW\")\nsample_tbl_view_df.show(5)  # Display the first 5 rows\n# Displaying the DataFrame (Lazy Evaluation, Only Shows Query)\n# result = session.sql(\"\"\"\n#     SELECT \n#         COL_A, \n#         typeof(COL_A) AS column_type \n#     FROM SAMPLE_TBL_SNOW_VIEW \n#     LIMIT 1;\n# \"\"\")\n# print(result.collect())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4deeef33-1c6b-4094-a3e7-e0e0fed3d921",
   "metadata": {
    "language": "sql",
    "name": "SNOWPARK_CAST_VIEW_OUTPUT"
   },
   "outputs": [],
   "source": "-- Verifying the Precision and Scale of the Downcasted Column in the View --\nSELECT \n    COLUMN_NAME, \n    DATA_TYPE, \n    NUMERIC_PRECISION, \n    NUMERIC_SCALE \nFROM \n    INFORMATION_SCHEMA.COLUMNS \nWHERE \n    TABLE_NAME = 'SAMPLE_TBL_SNOW_VIEW' \n    AND COLUMN_NAME = 'COL_A';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0acbecd-3b2a-4332-a717-aef303154462",
   "metadata": {
    "name": "SNOW_DECIMAL_OVERFLOW",
    "collapsed": false
   },
   "source": "## Testing the largest and smallest value in Snowflake DECIMAL.\n#### Overflow on downcast - intentionally identify and specify a value(Value Clamping) or throw an error."
  },
  {
   "cell_type": "code",
   "id": "f3cdd22f-f95d-4a3b-9a8c-708fe4b87ad8",
   "metadata": {
    "language": "sql",
    "name": "max_decimal_test"
   },
   "outputs": [],
   "source": "-- Testing the largest exact integer and decimal in Snowflake\nSELECT \n    999999999999999::DECIMAL(38,0) AS max_dec,\n    9999999999999999::DECIMAL(38,0) AS max_dec_mag_plus_one,\n   -999999999999999999999999999999999999::DECIMAL(38,0) AS min_dec,\n    TRY_CAST((9999999999999999999999999999999999999 + 1) AS DECIMAL(38,0)) AS max_plus_one;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "650f12bc-b747-4c8b-a906-a3415577836e",
   "metadata": {
    "language": "python",
    "name": "max_decimal_test_py"
   },
   "outputs": [],
   "source": "query = \"\"\"\nSELECT \n    999999999999999::DECIMAL(38,0) AS max_dec,\n    9999999999999999::DECIMAL(38,0) AS max_dec_mag_plus_one,\n    -999999999999999999999999999999999999::DECIMAL(38,0) AS min_dec,\n    TRY_CAST((9999999999999999999999999999999999999 + 1) AS DECIMAL(38,0)) AS max_plus_one;\n\"\"\"\n\n# Executing the query\nresult_df = session.sql(query).collect()\n\n# Displaying the result\nfor row in result_df:\n    print(row)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d661362-2c61-42b0-8449-d912eb10296d",
   "metadata": {
    "language": "sql",
    "name": "DECIMAL_NO_ARG_TEST"
   },
   "outputs": [],
   "source": "-- Testing the largest DECIMAL value in Snowflake\n-- SELECT \n--     CAST('999999999999999' AS DECIMAL(38,0)) AS max_decimal;\n\nCREATE OR REPLACE VIEW SAMPLE_TBL_VIEW AS \nSELECT \n    CAST('999999999999999' AS DECIMAL) AS max_decimal;\n\n-- Verifying the Precision and Scale of the Downcasted Column in the View --\nSELECT \n    COLUMN_NAME, \n    DATA_TYPE, \n    NUMERIC_PRECISION, \n    NUMERIC_SCALE \nFROM \n    INFORMATION_SCHEMA.COLUMNS \nWHERE \n    TABLE_NAME = 'SAMPLE_TBL_VIEW';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38355ad5-f9cc-406a-a9c1-513f9898abc2",
   "metadata": {
    "language": "sql",
    "name": "DECIMAL_NO_ARG_CAST_TEST"
   },
   "outputs": [],
   "source": "SELECT \n    CAST(COL_A AS DECIMAL(35,2)) AS column_downcasted\nFROM \n    SAMPLE_TBL;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88b2d6d1-8407-415a-a7e6-920c3fc3d4fe",
   "metadata": {
    "language": "python",
    "name": "snow_truncation"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\n# Define the exact max and min safe range for DECIMAL(37,1)\nmax_decimal_value =  9999999999999.99\nmin_decimal_value = -9999999999999.99\n\n# # Ensure values are within the safe range using LEAST and GREATEST\nsnow_df_input = session.table(\"SAMPLE_TBL_MAT_VIEW\").select(\n    F.sql_expr(f\"CAST(LEAST(GREATEST(ROUND(col_a, 1), {min_decimal_value}), {max_decimal_value}) AS DECIMAL(37,2))\").alias(\"col_a\"),\n    F.sql_expr(f\"CAST(LEAST(GREATEST(ROUND(col_b, 1), {min_decimal_value}), {max_decimal_value}) AS DECIMAL(37,2))\").alias(\"col_b\")\n)\n\nsnow_df_input.collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3349943e-0fe1-4878-a7f3-2a4766decf3a",
   "metadata": {
    "name": "SNOWFLAKE_MODEL",
    "collapsed": false
   },
   "source": "## Logic for Multiply() into Snowflake Registry\n\n### BNY Team - PREP\\_PRECISION() can be any Python function using data already in memory. The code above clamps within Snowflake, but preprocessing with Pandas in Python runtime is also efficient. Treat the decorated function in the Model Object as a driver, calling any custom function you write.\n"
  },
  {
   "cell_type": "code",
   "id": "ecf1baf7-4725-40fd-a447-98a0bbc5160f",
   "metadata": {
    "language": "python",
    "name": "ARROW_MUL_CLASS",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\nfrom snowflake.ml.model import custom_model\nimport pyarrow as pa\nimport pyarrow.compute as pc\nimport pandas as pd\n\n# Name of the class\nclass arrow_decimal_fast(custom_model.CustomModel):\n    def prep_precision(self, input_df: pd.DataFrame) -> pd.DataFrame:\n        # Define the exact max and min safe range for DECIMAL(37,1)\n        max_decimal_value = 9999999999999999999999999999999999999\n        min_decimal_value = -9999999999999999999999999999999999999\n        decimal_precision = 37\n        decimal_scale = 18\n\n        # Apply clamping to maintain DECIMAL precision\n        input_df[\"COL_A\"] = input_df[\"COL_A\"].clip(lower=min_decimal_value, upper=max_decimal_value)\n        input_df[\"COL_B\"] = input_df[\"COL_B\"].clip(lower=min_decimal_value, upper=max_decimal_value)\n        return input_df\n\n    @custom_model.inference_api\n    def arrow_multiply(self, X: pd.DataFrame) -> pd.DataFrame:\n        # Preparing input data with DECIMAL precision\n        X = self.prep_precision(X)\n        \n        # Convert columns to PyArrow Decimal\n        col_a = pa.array(X[\"COL_A\"], type=pa.decimal128(37, 1))\n        col_b = pa.array(X[\"COL_B\"], type=pa.decimal128(37, 1))\n\n        # Perform precise DECIMAL multiplication\n        product = pc.multiply(col_a, col_b)\n        \n        # Convert result to pandas DataFrame\n        result_df = pd.DataFrame({\"PRODUCT\": product.to_pandas()})\n        return result_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0393376-1112-41cf-ab13-6b125e059d79",
   "metadata": {
    "language": "python",
    "name": "ARROW_MUL_REGISTER"
   },
   "outputs": [],
   "source": "# Instantiate model with context and register\nmodel_context = custom_model.ModelContext()\nmodel = arrow_decimal_fast(model_context)\n\n# Sample input for schema validation\nsample_input = pd.DataFrame({\n    \"COL_A\": [9999999999.9],\n    \"COL_B\": [2.0]\n})\n\n# Register the model\nregistry = Registry(session=session)\nregistry.log_model(\n    model=model,\n    model_name=\"pyarrow_decimal_fast\",\n    conda_dependencies=[\"pyarrow\"],\n    version_name=\"v1\",\n    sample_input_data=sample_input,\n    target_platform=[\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n    options={\"overwrite\": True},\n)\n\n# Running the model (Example)\nresult = model.arrow_multiply(sample_input)\nprint(result)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14463228-e963-45d1-a463-4bb6c1a41bb9",
   "metadata": {
    "name": "MODEL_SERVING_SERVICE",
    "collapsed": false
   },
   "source": "## Model Inferencing Service to avoid UDxF overhead"
  },
  {
   "cell_type": "code",
   "id": "acad6a35-2e34-452a-a54b-f119fb2c7966",
   "metadata": {
    "language": "python",
    "name": "INFERNCE_SPCS_PREP"
   },
   "outputs": [],
   "source": "image_repo_name = \"arrow_inference_image\" # needs to be created\ncp_name = \"E2E_ML_GPU_NV_S\"               # compute_pool should be created\nnum_spcs_nodes = '2'\nservice_name = 'ARROW_DEC_SERVICE'\n\ncurrent_database = session.get_current_database().replace('\"', '')\ncurrent_schema = session.get_current_schema().replace('\"', '')\nextended_image_repo_name = f\"{current_database}.DEFAULT_SCHEMA.{image_repo_name}\"\nextended_service_name = f'{current_database}.DEFAULT_SCHEMA.{service_name}'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22e4c2b5-ac83-4721-8302-341ce19a9175",
   "metadata": {
    "language": "sql",
    "name": "DROP_SERVICE"
   },
   "outputs": [],
   "source": "DROP SERVICE IF EXISTS {{service_name}};",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "922aa630-0622-4821-b936-c9763cccd68d",
   "metadata": {
    "language": "python",
    "name": "CREATE_SERVICE"
   },
   "outputs": [],
   "source": "mv_base.create_service(\n    service_name=extended_service_name,\n    service_compute_pool=cp_name,\n    image_repo=extended_image_repo_name,\n    ingress_enabled=True,\n    max_instances=int(num_spcs_nodes),\n    build_external_access_integration=\"ALLOW_ALL_INTEGRATION\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "424f4b8b-0654-4d82-b9d8-99bb627c55de",
   "metadata": {
    "language": "sql",
    "name": "PRINT_SERVICES"
   },
   "outputs": [],
   "source": "SHOW SERVICES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64bae109-e60e-469a-92c3-6eb4386d8fcf",
   "metadata": {
    "language": "python",
    "name": "SHOW_SERVICES"
   },
   "outputs": [],
   "source": "# mv_base = model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").version(\"XGB_GPU_DIST\")\nmv_base = model_registry.get_model(f\"MORTGAGE_LENDING_MLOPS_{VERSION_NUM}\").version(\"v1\")\nmv_base.list_services()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3ec82ce-a504-483f-83ca-291d1815539e",
   "metadata": {
    "language": "python",
    "name": "RUN_SERVICE"
   },
   "outputs": [],
   "source": "mv_base.run(test, \n            function_name = \"arrow_multiply\", \n            service_name = \"DEFAULT_SCHEMA.ARROW_DEC_SERVICE\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c7eb3a0-21d3-423b-ba36-f05d054111c5",
   "metadata": {
    "name": "TESTING_DEBUGGING",
    "collapsed": false
   },
   "source": "## Further Testing / Debugging"
  },
  {
   "cell_type": "code",
   "id": "0f7cab19-479d-4b52-b1fa-fc336984ebcc",
   "metadata": {
    "language": "python",
    "name": "debug_1",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\ndef safe_numeric_37_1(column_name):\n    \"\"\"\n    Efficiently ensures a column is safely cast to NUMERIC(37,1) without overflow.\n    - Caps values to the safe range of NUMERIC(37,1).\n    - Uses standard CAST because range is strictly controlled.\n    \"\"\"\n    max_value = 9999999999999999999999999999999999.9\n    min_value = -999999999999999999999999999999999.9\n    \n    # Applying safe cast with range control using standard CAST\n    return F.cast(\n        F.least(\n            F.greatest(F.round(F.col(column_name), 1), F.lit(min_value)),\n            F.lit(max_value)\n        ), \n        \"DECIMAL(37,1)\"  # Using NUMBER because DECIMAL/Numeric is not fully supported\n    )\n\n# Apply this to your Snowflake DataFrame using Snowpark API\nsnow_df_input = session.table(\"SAMPLE_TBL\").select(\n    safe_numeric_37_1(\"col_a\").alias(\"col_a\"),\n    safe_numeric_37_1(\"col_b\").alias(\"col_b\")\n)\n\n# Collect the data without overflow\nresult_df = snow_df_input.collect()\nprint(result_df)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe7aa85b-c150-4e2d-9db1-923772002ba8",
   "metadata": {
    "language": "python",
    "name": "debug_print"
   },
   "outputs": [],
   "source": "arrow_tbl = snow_df_input.to_arrow()\nprint(arrow_tbl)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13629cdf-4896-4c0b-8006-9d705c85b2d4",
   "metadata": {
    "language": "python",
    "name": "debug_2"
   },
   "outputs": [],
   "source": "# Step 2: Multiply and truncate result to decimal128(38, 10)\na = arrow_tbl.column(\"COL_A\")\nb = arrow_tbl.column(\"COL_B\")\nproduct = pc.multiply(a, b)\nprint(product)\n# rounded = pc.cast((pc.cast(product, pa.decimal256(75, 1))), pa.decimal128(38, 1))",
   "execution_count": null
  }
 ]
}